{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import numpy as np\n",
    "df = pd.read_csv('data/diabetic_data.csv', encoding = 'latin1',low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('data/diabetic_data.csv', encoding = 'latin1',low_memory=False)\n",
    "df.readmitted[df.readmitted == 'NO' ] = 0\n",
    "df.readmitted[df.readmitted == '<30' ] = 1\n",
    "df = df.drop(df[df.readmitted == '>30'].index)\n",
    "df.drop(['encounter_id', 'patient_nbr', 'weight', 'payer_code', 'medical_specialty'], axis=1, inplace=True)\n",
    "df.dropna(axis=1, how='all')\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, stratify=df['readmitted'])\n",
    "#X = pd.get_dummies(X)\n",
    "\n",
    "\n",
    "sample_train = df_train.sample(frac=0.1)\n",
    "sample_test = df_test.sample(frac=0.1)\n",
    "\n",
    "y_train_sample = sample_train['readmitted']\n",
    "y_test_sample = sample_test['readmitted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "int_categorical_headers = [\n",
    "'admission_type_id',\n",
    "'discharge_disposition_id',\n",
    "'admission_source_id',\n",
    "]\n",
    "numeric_headers = [\n",
    "    \"time_in_hospital\", \n",
    "    \"num_lab_procedures\", \n",
    "    \"num_procedures\", \n",
    "    \"num_medications\", \n",
    "    \"number_outpatient\", \n",
    "    \"number_emergency\", \n",
    "    \"number_inpatient\", \n",
    "    \"number_diagnoses\",\n",
    "]\n",
    "categorical_headers = [\n",
    " 'diag_3',\n",
    " 'diag_2',\n",
    " 'diag_1',\n",
    " 'race',\n",
    " 'gender',\n",
    " 'age',\n",
    " 'max_glu_serum',\n",
    " 'A1Cresult',\n",
    " 'metformin',\n",
    " 'repaglinide',\n",
    " 'nateglinide',\n",
    " 'chlorpropamide',\n",
    " 'glimepiride',\n",
    " 'acetohexamide',\n",
    " 'glipizide',\n",
    " 'glyburide',\n",
    " 'tolbutamide',\n",
    " 'pioglitazone',\n",
    " 'rosiglitazone',\n",
    " 'acarbose',\n",
    " 'miglitol',\n",
    " 'troglitazone',\n",
    " 'tolazamide',\n",
    " 'examide',\n",
    " 'citoglipton',\n",
    " 'insulin',\n",
    " 'glyburide-metformin',\n",
    " 'glipizide-metformin',\n",
    " 'glimepiride-pioglitazone',\n",
    " 'metformin-rosiglitazone',\n",
    " 'metformin-pioglitazone',\n",
    " 'change',\n",
    " 'diabetesMed']\n",
    "\n",
    "df_train.replace(to_replace=' ?',value=np.nan, inplace=True)\n",
    "df_train.dropna(inplace=True)\n",
    "df_train.reset_index()\n",
    "\n",
    "df_test.replace(to_replace=' ?',value=np.nan, inplace=True)\n",
    "df_test.dropna(inplace=True)\n",
    "df_test.reset_index()\n",
    "\n",
    "encoders = dict()\n",
    "\n",
    "\n",
    "\n",
    "for col in categorical_headers:\n",
    "    df[col] = df[col].str.strip()\n",
    "    df_train[col] = df_train[col].str.strip()\n",
    "    df_test[col] = df_test[col].str.strip()\n",
    "    sample_train[col] = sample_train[col].str.strip()\n",
    "    sample_test[col] = sample_test[col].str.strip()\n",
    "    \n",
    "\n",
    "    encoders[col] = LabelEncoder()\n",
    "    df[col+'_int'] = encoders[col].fit_transform(df[col])\n",
    "    df_train[col+'_int'] = encoders[col].transform(df_train[col])\n",
    "    df_test[col+'_int'] = encoders[col].transform(df_test[col])\n",
    "    sample_train[col+'_int'] = encoders[col].transform(sample_train[col])\n",
    "    sample_test[col+'_int'] = encoders[col].transform(sample_test[col])\n",
    "    \n",
    "    \n",
    "    \n",
    "for col in int_categorical_headers:\n",
    "    df[col+'_int'] = df[col]\n",
    "    df_train[col+'_int'] = df_train[col]\n",
    "    df_test[col+'_int'] = df_test[col]\n",
    "    sample_test[col+'_int'] = sample_test[col]\n",
    "    sample_train[col+'_int'] = sample_train[col]\n",
    "\n",
    "for col in numeric_headers:\n",
    "    df_train[col] = df_train[col].astype(np.float)\n",
    "    df_test[col] = df_test[col].astype(np.float)\n",
    "    sample_train[col] = sample_train[col].astype(np.float)\n",
    "    sample_test[col] = sample_test[col].astype(np.float)\n",
    "    df[col] = df[col].astype(np.float)\n",
    "    \n",
    "    ss = StandardScaler()\n",
    "    df[col] = ss.fit_transform(df[col].values.reshape(-1, 1))\n",
    "    df_train[col] = ss.transform(df_train[col].values.reshape(-1, 1))\n",
    "    df_test[col] = ss.transform(df_test[col].values.reshape(-1, 1))\n",
    "    sample_train[col] = ss.transform(sample_train[col].values.reshape(-1, 1))\n",
    "    sample_test[col] = ss.transform(sample_test[col].values.reshape(-1, 1))\n",
    "\n",
    "    \n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics as mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Input\n",
    "from keras.layers import Embedding, Flatten, Merge, concatenate\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting work on the actual deep and wide network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_network(IN_Train, IN_Test, deep=None):\n",
    "    categorical_headers_ints = [x+'_int' for x in categorical_headers]\n",
    "    master_numeric_headers = numeric_headers + int_categorical_headers\n",
    "    df_num =  IN_Train[master_numeric_headers].values\n",
    "    X_train_num =  IN_Train[master_numeric_headers].values\n",
    "    X_test_num =  IN_Test[master_numeric_headers].values\n",
    "    y_train = IN_Train['readmitted'].values.astype(np.int)\n",
    "    y_test = IN_Test['readmitted'].values.astype(np.int)\n",
    "\n",
    "    import tensorflow as tf\n",
    "    with tf.device('/gpu:0'):\n",
    "        embed_branches = []\n",
    "        X_ints_train = []\n",
    "        X_ints_test = []\n",
    "        all_inputs = []\n",
    "        all_branch_outputs = []\n",
    "\n",
    "        for cols in cross_columns:\n",
    "            # encode crossed columns as ints for the embedding\n",
    "            enc = LabelEncoder()\n",
    "\n",
    "            # create crossed labels\n",
    "            # needs to be commented better, Eric!\n",
    "            X_crossed_train = IN_Train[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "            X_crossed_test = IN_Test[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "\n",
    "            enc.fit(np.hstack((X_crossed_train.values,  X_crossed_test.values)))\n",
    "            X_crossed_train = enc.transform(X_crossed_train)\n",
    "            X_crossed_test = enc.transform(X_crossed_test)\n",
    "            X_ints_train.append( X_crossed_train )\n",
    "            X_ints_test.append( X_crossed_test )\n",
    "\n",
    "            # get the number of categories\n",
    "            N = max(X_ints_train[-1]+1) # same as the max(df_train[col])\n",
    "\n",
    "            # create embedding branch from the number of categories\n",
    "            inputs = Input(shape=(1,),dtype='int32', name = '_'.join(cols))\n",
    "            all_inputs.append(inputs)\n",
    "            x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "            x = Flatten()(x)\n",
    "            all_branch_outputs.append(x)\n",
    "\n",
    "        # merge the branches together\n",
    "        wide_branch = concatenate(all_branch_outputs)\n",
    "\n",
    "        # reset this input branch\n",
    "        all_branch_outputs = []\n",
    "        # add in the embeddings\n",
    "        for col in categorical_headers_ints:\n",
    "            # encode as ints for the embedding\n",
    "            X_ints_train.append( IN_Train[col].values )\n",
    "            X_ints_test.append( IN_Test[col].values )\n",
    "\n",
    "            # get the number of categories\n",
    "            N = max(X_ints_train[-1]+1) # same as the max(df_train[col])\n",
    "\n",
    "            # create embedding branch from the number of categories\n",
    "            inputs = Input(shape=(1,),dtype='int32', name=col)\n",
    "            all_inputs.append(inputs)\n",
    "            x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "            x = Flatten()(x)\n",
    "            all_branch_outputs.append(x)\n",
    "\n",
    "        # also get a dense branch of the numeric features\n",
    "        all_inputs.append(Input(shape=(X_train_num.shape[1],),sparse=False,name='numeric_data'))\n",
    "        x = Dense(units=20, activation='relu')(all_inputs[-1])\n",
    "        all_branch_outputs.append( x )\n",
    "\n",
    "        # merge the branches together\n",
    "        deep_branch = concatenate(all_branch_outputs)\n",
    "        deep_branch = Dense(units=50,activation='relu')(deep_branch)\n",
    "        if deep == \"start\":\n",
    "            deep_branch = Dense(units=40,activation='relu')(deep_branch)\n",
    "            deep_branch = Dense(units=30,activation='relu')(deep_branch)\n",
    "            deep_branch = Dense(units=20,activation='relu')(deep_branch)\n",
    "        deep_branch = Dense(units=10,activation='relu')(deep_branch)\n",
    "\n",
    "        final_branch = concatenate([wide_branch, deep_branch])\n",
    "        if deep == \"final\":\n",
    "            final_branch = Dense(units=50,activation='sigmoid')(final_branch)\n",
    "        final_branch = Dense(units=1,activation='sigmoid')(final_branch)\n",
    "        model = None\n",
    "        model = Model(inputs=all_inputs, outputs=final_branch)\n",
    "        model.compile(optimizer='adagrad',\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        model.fit(X_ints_train+ [X_train_num],\n",
    "                y_train, epochs=1, batch_size=32, verbose=1)\n",
    "        return model\n",
    "        #yhat = np.round(model.predict(X_ints_test + [X_test_num]))\n",
    "        #print(mt.confusion_matrix(y_test,yhat),mt.accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THIS SHIT WORKS\n",
    "## This is for 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 3\n",
    "\n",
    "model_list1 = []\n",
    "yhat_list1 = []\n",
    "y_test_list1 = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
    "for iter_num, (train_indices, test_indices) in enumerate(skf.split(df_train.drop(['readmitted'], axis = 1), df_train['readmitted'])):\n",
    "    df_train_cv = df_train.iloc[train_indices]\n",
    "    df_test_cv = df_train.iloc[test_indices]\n",
    "    embed_branches = []\n",
    "    X_ints_train = []\n",
    "    X_ints_test = []\n",
    "    all_inputs = []\n",
    "    all_branch_outputs = []\n",
    "    \n",
    "    categorical_headers_ints = [x+'_int' for x in categorical_headers]\n",
    "    master_numeric_headers = numeric_headers + int_categorical_headers\n",
    "    df_num =  df_train_cv[master_numeric_headers].values\n",
    "    X_train_num =  df_train_cv[master_numeric_headers].values\n",
    "    X_test_num =  df_test_cv[master_numeric_headers].values\n",
    "    y_train = df_train_cv['readmitted'].values.astype(np.int)\n",
    "    y_test = df_test_cv['readmitted'].values.astype(np.int)\n",
    "\n",
    "    for cols in cross_columns:\n",
    "        # encode crossed columns as ints for the embedding\n",
    "        enc = LabelEncoder()\n",
    "\n",
    "        # create crossed labels\n",
    "        # needs to be commented better, Eric!\n",
    "        X_crossed_train = df_train_cv[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "        X_crossed_test = df_test_cv[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "\n",
    "        enc.fit(np.hstack((X_crossed_train.values,  X_crossed_test.values)))\n",
    "        X_crossed_train = enc.transform(X_crossed_train)\n",
    "        X_crossed_test = enc.transform(X_crossed_test)\n",
    "        X_ints_train.append( X_crossed_train )\n",
    "        X_ints_test.append( X_crossed_test )\n",
    "\n",
    "        # get the number of categories\n",
    "        N = max(X_ints_train[-1]+1) # same as the max(df_train[col])\n",
    "\n",
    "        # create embedding branch from the number of categories\n",
    "        inputs = Input(shape=(1,),dtype='int32', name = '_'.join(cols))\n",
    "        all_inputs.append(inputs)\n",
    "        x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "        x = Flatten()(x)\n",
    "        all_branch_outputs.append(x)\n",
    "\n",
    "    # merge the branches together\n",
    "    wide_branch = concatenate(all_branch_outputs)\n",
    "\n",
    "    # reset this input branch\n",
    "    all_branch_outputs = []\n",
    "    # add in the embeddings\n",
    "    for col in categorical_headers_ints:\n",
    "        # encode as ints for the embedding\n",
    "        X_ints_train.append( df_train_cv[col].values )\n",
    "        X_ints_test.append( df_test_cv[col].values )\n",
    "\n",
    "        # get the number of categories\n",
    "        N = max(X_ints_train[-1]+1) # same as the max(df_train[col])\n",
    "\n",
    "        # create embedding branch from the number of categories\n",
    "        inputs = Input(shape=(1,),dtype='int32', name=col)\n",
    "        all_inputs.append(inputs)\n",
    "        x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "        x = Flatten()(x)\n",
    "        all_branch_outputs.append(x)\n",
    "\n",
    "    # also get a dense branch of the numeric features\n",
    "    all_inputs.append(Input(shape=(X_train_num.shape[1],),sparse=False,name='numeric_data'))\n",
    "    x = Dense(units=20, activation='relu')(all_inputs[-1])\n",
    "    all_branch_outputs.append( x )\n",
    "\n",
    "    # merge the branches together\n",
    "    deep_branch = concatenate(all_branch_outputs)\n",
    "    deep_branch = Dense(units=50,activation='relu')(deep_branch)\n",
    "    deep_branch = Dense(units=10,activation='relu')(deep_branch)\n",
    "\n",
    "    final_branch = concatenate([wide_branch, deep_branch])\n",
    "    final_branch = Dense(units=1,activation='sigmoid')(final_branch)\n",
    "\n",
    "    modeltemp = Model(inputs=all_inputs, outputs=final_branch)\n",
    "\n",
    "    modeltemp.compile(optimizer='adagrad',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    modeltemp.fit(X_ints_train+ [X_train_num],\n",
    "            y_train, epochs=1, batch_size=32, verbose=1)\n",
    "    model_list1.append(modeltemp)\n",
    "    yhat_temp = np.round(modeltemp.predict(X_ints_test + [X_test_num]))\n",
    "    yhat_list1.append(yhat_temp)\n",
    "    y_test_list1.append(y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores1 = []\n",
    "for i in range(0, len(yhat_list1)):\n",
    "    print(mt.confusion_matrix(y_test_list1[i],yhat_list1[i]), mt.recall_score(y_test_list1[i],yhat_list1[i]))\n",
    "    scores1.append(mt.recall_score(y_test_list1[i],yhat_list1[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "i = list(range(1,num_folds+1))\n",
    "plt.bar(i,scores1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list2 = []\n",
    "yhat_list2 = []\n",
    "y_test_list2 = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
    "for iter_num, (train_indices, test_indices) in enumerate(skf.split(df_train.drop(['readmitted'], axis = 1), df_train['readmitted'])):\n",
    "    df_train_cv = df_train.iloc[train_indices]\n",
    "    df_test_cv = df_train.iloc[test_indices]\n",
    "    embed_branches = []\n",
    "    X_ints_train = []\n",
    "    X_ints_test = []\n",
    "    all_inputs = []\n",
    "    all_branch_outputs = []\n",
    "    \n",
    "    categorical_headers_ints = [x+'_int' for x in categorical_headers]\n",
    "    master_numeric_headers = numeric_headers + int_categorical_headers\n",
    "    df_num =  df_train_cv[master_numeric_headers].values\n",
    "    X_train_num =  df_train_cv[master_numeric_headers].values\n",
    "    X_test_num =  df_test_cv[master_numeric_headers].values\n",
    "    y_train = df_train_cv['readmitted'].values.astype(np.int)\n",
    "    y_test = df_test_cv['readmitted'].values.astype(np.int)\n",
    "\n",
    "    for cols in cross_columns:\n",
    "        # encode crossed columns as ints for the embedding\n",
    "        enc = LabelEncoder()\n",
    "\n",
    "        # create crossed labels\n",
    "        # needs to be commented better, Eric!\n",
    "        X_crossed_train = df_train_cv[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "        X_crossed_test = df_test_cv[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "\n",
    "        enc.fit(np.hstack((X_crossed_train.values,  X_crossed_test.values)))\n",
    "        X_crossed_train = enc.transform(X_crossed_train)\n",
    "        X_crossed_test = enc.transform(X_crossed_test)\n",
    "        X_ints_train.append( X_crossed_train )\n",
    "        X_ints_test.append( X_crossed_test )\n",
    "\n",
    "        # get the number of categories\n",
    "        N = max(X_ints_train[-1]+1) # same as the max(df_train[col])\n",
    "\n",
    "        # create embedding branch from the number of categories\n",
    "        inputs = Input(shape=(1,),dtype='int32', name = '_'.join(cols))\n",
    "        all_inputs.append(inputs)\n",
    "        x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "        x = Flatten()(x)\n",
    "        all_branch_outputs.append(x)\n",
    "\n",
    "    # merge the branches together\n",
    "    wide_branch = concatenate(all_branch_outputs)\n",
    "\n",
    "    # reset this input branch\n",
    "    all_branch_outputs = []\n",
    "    # add in the embeddings\n",
    "    for col in categorical_headers_ints:\n",
    "        # encode as ints for the embedding\n",
    "        X_ints_train.append( df_train_cv[col].values )\n",
    "        X_ints_test.append( df_test_cv[col].values )\n",
    "\n",
    "        # get the number of categories\n",
    "        N = max(X_ints_train[-1]+1) # same as the max(df_train[col])\n",
    "\n",
    "        # create embedding branch from the number of categories\n",
    "        inputs = Input(shape=(1,),dtype='int32', name=col)\n",
    "        all_inputs.append(inputs)\n",
    "        x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "        x = Flatten()(x)\n",
    "        all_branch_outputs.append(x)\n",
    "\n",
    "    # also get a dense branch of the numeric features\n",
    "    all_inputs.append(Input(shape=(X_train_num.shape[1],),sparse=False,name='numeric_data'))\n",
    "    x = Dense(units=20, activation='relu')(all_inputs[-1])\n",
    "    all_branch_outputs.append( x )\n",
    "\n",
    "    # merge the branches together\n",
    "    deep_branch = concatenate(all_branch_outputs)\n",
    "    deep_branch = Dense(units=50,activation='relu')(deep_branch)\n",
    "    deep_branch = Dense(units=40,activation='relu')(deep_branch)\n",
    "    deep_branch = Dense(units=30,activation='relu')(deep_branch)\n",
    "    deep_branch = Dense(units=20,activation='relu')(deep_branch)\n",
    "    deep_branch = Dense(units=10,activation='relu')(deep_branch)\n",
    "\n",
    "    final_branch = concatenate([wide_branch, deep_branch])\n",
    "    final_branch = Dense(units=1,activation='sigmoid')(final_branch)\n",
    "\n",
    "    modeltemp = Model(inputs=all_inputs, outputs=final_branch)\n",
    "\n",
    "    modeltemp.compile(optimizer='adagrad',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    modeltemp.fit(X_ints_train+ [X_train_num],\n",
    "            y_train, epochs=1, batch_size=32, verbose=1)\n",
    "    model_list2.append(modeltemp)\n",
    "    yhat_temp = np.round(modeltemp.predict(X_ints_test + [X_test_num]))\n",
    "    yhat_list2.append(yhat_temp)\n",
    "    y_test_list2.append(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list3 = []\n",
    "yhat_list3 = []\n",
    "y_test_list3 = []\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
    "for iter_num, (train_indices, test_indices) in enumerate(skf.split(df_train.drop(['readmitted'], axis = 1), df_train['readmitted'])):\n",
    "    df_train_cv = df_train.iloc[train_indices]\n",
    "    df_test_cv = df_train.iloc[test_indices]\n",
    "    embed_branches = []\n",
    "    X_ints_train = []\n",
    "    X_ints_test = []\n",
    "    all_inputs = []\n",
    "    all_branch_outputs = []\n",
    "    \n",
    "    categorical_headers_ints = [x+'_int' for x in categorical_headers]\n",
    "    master_numeric_headers = numeric_headers + int_categorical_headers\n",
    "    df_num =  df_train_cv[master_numeric_headers].values\n",
    "    X_train_num =  df_train_cv[master_numeric_headers].values\n",
    "    X_test_num =  df_test_cv[master_numeric_headers].values\n",
    "    y_train = df_train_cv['readmitted'].values.astype(np.int)\n",
    "    y_test = df_test_cv['readmitted'].values.astype(np.int)\n",
    "\n",
    "    for cols in cross_columns:\n",
    "        # encode crossed columns as ints for the embedding\n",
    "        enc = LabelEncoder()\n",
    "\n",
    "        # create crossed labels\n",
    "        # needs to be commented better, Eric!\n",
    "        X_crossed_train = df_train_cv[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "        X_crossed_test = df_test_cv[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "\n",
    "        enc.fit(np.hstack((X_crossed_train.values,  X_crossed_test.values)))\n",
    "        X_crossed_train = enc.transform(X_crossed_train)\n",
    "        X_crossed_test = enc.transform(X_crossed_test)\n",
    "        X_ints_train.append( X_crossed_train )\n",
    "        X_ints_test.append( X_crossed_test )\n",
    "\n",
    "        # get the number of categories\n",
    "        N = max(X_ints_train[-1]+1) # same as the max(df_train[col])\n",
    "\n",
    "        # create embedding branch from the number of categories\n",
    "        inputs = Input(shape=(1,),dtype='int32', name = '_'.join(cols))\n",
    "        all_inputs.append(inputs)\n",
    "        x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "        x = Flatten()(x)\n",
    "        all_branch_outputs.append(x)\n",
    "\n",
    "    # merge the branches together\n",
    "    wide_branch = concatenate(all_branch_outputs)\n",
    "\n",
    "    # reset this input branch\n",
    "    all_branch_outputs = []\n",
    "    # add in the embeddings\n",
    "    for col in categorical_headers_ints:\n",
    "        # encode as ints for the embedding\n",
    "        X_ints_train.append( df_train_cv[col].values )\n",
    "        X_ints_test.append( df_test_cv[col].values )\n",
    "\n",
    "        # get the number of categories\n",
    "        N = max(X_ints_train[-1]+1) # same as the max(df_train[col])\n",
    "\n",
    "        # create embedding branch from the number of categories\n",
    "        inputs = Input(shape=(1,),dtype='int32', name=col)\n",
    "        all_inputs.append(inputs)\n",
    "        x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "        x = Flatten()(x)\n",
    "        all_branch_outputs.append(x)\n",
    "\n",
    "    # also get a dense branch of the numeric features\n",
    "    all_inputs.append(Input(shape=(X_train_num.shape[1],),sparse=False,name='numeric_data'))\n",
    "    x = Dense(units=20, activation='relu')(all_inputs[-1])\n",
    "    all_branch_outputs.append( x )\n",
    "\n",
    "    # merge the branches together\n",
    "    deep_branch = concatenate(all_branch_outputs)\n",
    "    deep_branch = Dense(units=50,activation='relu')(deep_branch)\n",
    "    deep_branch = Dense(units=10,activation='relu')(deep_branch)\n",
    "\n",
    "    final_branch = concatenate([wide_branch, deep_branch])\n",
    "    final_branch = Dense(units=50,activation='sigmoid')(final_branch)\n",
    "    final_branch = Dense(units=1,activation='sigmoid')(final_branch)\n",
    "\n",
    "    modeltemp = Model(inputs=all_inputs, outputs=final_branch)\n",
    "\n",
    "    modeltemp.compile(optimizer='adagrad',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    modeltemp.fit(X_ints_train+ [X_train_num],\n",
    "            y_train, epochs=1, batch_size=32, verbose=1)\n",
    "    model_list3.append(modeltemp)\n",
    "    yhat_temp = np.round(modeltemp.predict(X_ints_test + [X_test_num]))\n",
    "    yhat_list3.append(yhat_temp)\n",
    "    y_test_list3.append(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores1 = []\n",
    "\n",
    "for i in range(0, len(yhat_list1)):\n",
    "    print(mt.confusion_matrix(y_test_list1[i],yhat_list1[i]), mt.recall_score(y_test_list1[i],yhat_list1[i]))\n",
    "    scores1.append(mt.recall_score(y_test_list1[i],yhat_list1[i]))\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "i = list(range(1,num_folds+1))\n",
    "plt.bar(i,scores1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores2 = []\n",
    "\n",
    "for i in range(0, len(yhat_list2)):\n",
    "    print(mt.confusion_matrix(y_test_list2[i],yhat_list2[i]), mt.recall_score(y_test_list2[i],yhat_list2[i]))\n",
    "    scores2.append(mt.recall_score(y_test_list2[i],yhat_list2[i]))\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "i = list(range(1,num_folds+1))\n",
    "plt.bar(i,scores2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores3 = []\n",
    "\n",
    "for i in range(0, len(yhat_list3)):\n",
    "    print(mt.confusion_matrix(y_test_list3[i],yhat_list3[i]), mt.recall_score(y_test_list3[i],yhat_list3[i]))\n",
    "    scores3.append(mt.recall_score(y_test_list3[i],yhat_list3[i]))\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "i = list(range(1,num_folds+1))\n",
    "plt.bar(i,scores3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "yhat_list_sk = []\n",
    "y_test_list_sk = []\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
    "for iter_num, (train_indices, test_indices) in enumerate(skf.split(df_train.drop(['readmitted'], axis = 1), df_train['readmitted'])):\n",
    "    df_train_cv = df_train.iloc[train_indices]\n",
    "    df_test_cv = df_train.iloc[test_indices]\n",
    "    \n",
    "    df_dummies = pd.get_dummies(df_train_cv, columns=categorical_headers)\n",
    "\n",
    "    df_dummies_train, df_dummies_test = train_test_split(df_dummies, test_size=0.2, stratify=df_dummies['readmitted'])\n",
    "    y_train_sk = df_dummies_train['readmitted']\n",
    "    y_test_sk = df_dummies_test['readmitted']\n",
    "    df_dummies_train.drop(['readmitted'], axis=1, inplace=True)\n",
    "    df_dummies_test.drop(['readmitted'], axis=1, inplace=True)\n",
    "\n",
    "    clf = MLPClassifier()\n",
    "    clf.fit(X=df_dummies_train.values, y=np.asarray(y_train_sk.values, dtype=\"|S6\"))\n",
    "    yhat_sk = clf.predict(df_dummies_test.values)\n",
    "    yhat_list_sk.append(yhat_sk.astype(int))\n",
    "    y_test_list_sk.append(y_test_sk.values.astype(int))\n",
    "    print(mt.confusion_matrix(y_test_sk.values.astype(int),yhat_sk.astype(int)),mt.recall_score(y_test_sk.values.astype(int),yhat_sk.astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_sk = []\n",
    "\n",
    "for i in range(0, len(yhat_list_sk)):\n",
    "    print(mt.confusion_matrix(y_test_list_sk[i],yhat_list_sk[i]), mt.recall_score(y_test_list_sk[i],yhat_list_sk[i]))\n",
    "    scores_sk.append(mt.recall_score(y_test_list_sk[i],yhat_list_sk[i]))\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "i = list(range(1,num_folds+1))\n",
    "plt.bar(i,scores_sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CV, uses function above\n",
    "#in theory, this should treat df_train as our dataset, and split it, but it breaks when we use the cross val to split it\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, ShuffleSplit\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "#train_vals = np.array(df_train.values)\n",
    "model_list = []\n",
    "indices_list = []\n",
    "skf = StratifiedKFold(n_splits=2, shuffle=True)\n",
    "for iter_num, (train_indices, test_indices) in enumerate(skf.split(df_train.drop(['readmitted'], axis = 1), df_train['readmitted'])):\n",
    "    print(train, test)\n",
    "    indices_list.append([train_indices, test_indices])\n",
    "    model_list.append(run_network(df_train.iloc[train_indices], df_train.iloc[test_indices] )) #TODO: problem where we use the indicies from skf\n",
    "\n",
    "    \n",
    "    '''for train, test in skf.split(df_train.drop(['readmitted'], axis = 1), df_train['readmitted']):\n",
    "        print(train, test)\n",
    "        model_list.append(run_network(df_train[train], df_train[test])) #TODO: problem where we use the indicies from skf'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(model_list)):\n",
    "    print(df_train.iloc[indices_list[i][1]].values)\n",
    "    temp_list = []\n",
    "    for a in df_train.iloc[indices_list[i][1]].values:\n",
    "        temp_list.append(a)\n",
    "    yhat = np.round(model_list[i].predict(temp_list))\n",
    "    print(mt.confusion_matrix(df_train.iloc[indices_list[i][1]]['readmitted'],yhat),mt.accuracy_score(df_train.iloc[indices_list[i][1]]['readmitted'],yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of CV work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_numeric_headers = numeric_headers + int_categorical_headers\n",
    "categorical_headers_ints = [x+'_int' for x in categorical_headers]\n",
    "df_num =  df[numeric_headers].values\n",
    "X_train_num =  df_train[master_numeric_headers].values\n",
    "X_test_num =  df_test[master_numeric_headers].values\n",
    "y_train = df_train['readmitted'].values.astype(np.int)\n",
    "y_test = df_test['readmitted'].values.astype(np.int)\n",
    "\n",
    "X_train_num_sample =  sample_train[master_numeric_headers].values\n",
    "X_test_num_sample =  sample_test[master_numeric_headers].values\n",
    "y_train_sample = sample_train['readmitted'].values.astype(np.int)\n",
    "y_test_sample = sample_test['readmitted'].values.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(master_categorical_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_columns = [['gender','race'],\n",
    "                 ['age', 'diag_1'],\n",
    "                ['gender', 'diag_1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Standard Configuration Wide and Deep Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to create separate sequential models for each embedding\n",
    "import tensorflow as tf\n",
    "with tf.device('/GPU:0'):\n",
    "    embed_branches = []\n",
    "    X_ints_train = []\n",
    "    X_ints_test = []\n",
    "    all_inputs = []\n",
    "    all_branch_outputs = []\n",
    "\n",
    "    for cols in cross_columns:\n",
    "        # encode crossed columns as ints for the embedding\n",
    "        enc = LabelEncoder()\n",
    "\n",
    "        # create crossed labels\n",
    "        # needs to be commented better, Eric!\n",
    "        X_crossed_train = df_train[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "        X_crossed_test = df_test[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "\n",
    "        enc.fit(np.hstack((X_crossed_train.values,  X_crossed_test.values)))\n",
    "        X_crossed_train = enc.transform(X_crossed_train)\n",
    "        X_crossed_test = enc.transform(X_crossed_test)\n",
    "        X_ints_train.append( X_crossed_train )\n",
    "        X_ints_test.append( X_crossed_test )\n",
    "\n",
    "        # get the number of categories\n",
    "        N = max(X_ints_train[-1]+1) # same as the max(df_train[col])\n",
    "\n",
    "        # create embedding branch from the number of categories\n",
    "        inputs = Input(shape=(1,),dtype='int32', name = '_'.join(cols))\n",
    "        all_inputs.append(inputs)\n",
    "        x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "        x = Flatten()(x)\n",
    "        all_branch_outputs.append(x)\n",
    "\n",
    "    # merge the branches together\n",
    "    wide_branch = concatenate(all_branch_outputs)\n",
    "\n",
    "    # reset this input branch\n",
    "    all_branch_outputs = []\n",
    "    # add in the embeddings\n",
    "    for col in categorical_headers_ints:\n",
    "        # encode as ints for the embedding\n",
    "        X_ints_train.append( df_train[col].values )\n",
    "        X_ints_test.append( df_test[col].values )\n",
    "\n",
    "        # get the number of categories\n",
    "        N = max(X_ints_train[-1]+1) # same as the max(df_train[col])\n",
    "\n",
    "        # create embedding branch from the number of categories\n",
    "        inputs = Input(shape=(1,),dtype='int32', name=col)\n",
    "        all_inputs.append(inputs)\n",
    "        x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "        x = Flatten()(x)\n",
    "        all_branch_outputs.append(x)\n",
    "\n",
    "    # also get a dense branch of the numeric features\n",
    "    all_inputs.append(Input(shape=(X_train_num.shape[1],),sparse=False,name='numeric_data'))\n",
    "    x = Dense(units=20, activation='relu')(all_inputs[-1])\n",
    "    all_branch_outputs.append( x )\n",
    "\n",
    "    # merge the branches together\n",
    "    deep_branch = concatenate(all_branch_outputs)\n",
    "    deep_branch = Dense(units=50,activation='relu')(deep_branch)\n",
    "    deep_branch = Dense(units=10,activation='relu')(deep_branch)\n",
    "\n",
    "    final_branch = concatenate([wide_branch, deep_branch])\n",
    "    final_branch = Dense(units=1,activation='sigmoid')(final_branch)\n",
    "\n",
    "    model1 = Model(inputs=all_inputs, outputs=final_branch)\n",
    "\n",
    "    model1.compile(optimizer='adagrad',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model1.fit(X_ints_train+ [X_train_num],\n",
    "            y_train, epochs=10, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights_int = []\n",
    "for ind in model1.layers:\n",
    "    if 'embedding' in ind.name:\n",
    "        print(ind.name)\n",
    "        print(ind.get_weights())\n",
    "        weights_int.append(ind.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = np.round(model1.predict(X_ints_test + [X_test_num]))\n",
    "print(mt.confusion_matrix(y_test,yhat),mt.accuracy_score(y_test,yhat))\n",
    "print(X_ints_test) #+ [X_test_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.bar(range(len(per_fold_eval_criteria)),per_fold_eval_criteria)\n",
    "plt.ylim([min(per_fold_eval_criteria)-0.01,max(per_fold_eval_criteria)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(model1).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Deeper \"Deep Network\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to create separate sequential models for each embedding\n",
    "import tensorflow as tf\n",
    "with tf.device('/GPU:0'):\n",
    "    embed_branches = []\n",
    "    X_ints_train = []\n",
    "    X_ints_test = []\n",
    "    all_inputs = []\n",
    "    all_branch_outputs = []\n",
    "\n",
    "    for cols in cross_columns:\n",
    "        # encode crossed columns as ints for the embedding\n",
    "        enc = LabelEncoder()\n",
    "\n",
    "        # create crossed labels\n",
    "        # needs to be commented better, Eric!\n",
    "        X_crossed_train = df_train[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "        X_crossed_test = df_test[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "\n",
    "        enc.fit(np.hstack((X_crossed_train.values,  X_crossed_test.values)))\n",
    "        X_crossed_train = enc.transform(X_crossed_train)\n",
    "        X_crossed_test = enc.transform(X_crossed_test)\n",
    "        X_ints_train.append( X_crossed_train )\n",
    "        X_ints_test.append( X_crossed_test )\n",
    "\n",
    "        # get the number of categories\n",
    "        N = max(X_ints_train[-1]+1) # same as the max(df_train[col])\n",
    "\n",
    "        # create embedding branch from the number of categories\n",
    "        inputs = Input(shape=(1,),dtype='int32', name = '_'.join(cols))\n",
    "        all_inputs.append(inputs)\n",
    "        x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "        x = Flatten()(x)\n",
    "        all_branch_outputs.append(x)\n",
    "\n",
    "    # merge the branches together\n",
    "    wide_branch = concatenate(all_branch_outputs)\n",
    "\n",
    "    # reset this input branch\n",
    "    all_branch_outputs = []\n",
    "    # add in the embeddings\n",
    "    for col in categorical_headers_ints:\n",
    "        # encode as ints for the embedding\n",
    "        X_ints_train.append( df_train[col].values )\n",
    "        X_ints_test.append( df_test[col].values )\n",
    "\n",
    "        # get the number of categories\n",
    "        N = max(X_ints_train[-1]+1) # same as the max(df_train[col])\n",
    "\n",
    "        # create embedding branch from the number of categories\n",
    "        inputs = Input(shape=(1,),dtype='int32', name=col)\n",
    "        all_inputs.append(inputs)\n",
    "        x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "        x = Flatten()(x)\n",
    "        all_branch_outputs.append(x)\n",
    "\n",
    "    # also get a dense branch of the numeric features\n",
    "    all_inputs.append(Input(shape=(X_train_num.shape[1],),sparse=False,name='numeric_data'))\n",
    "    x = Dense(units=20, activation='relu')(all_inputs[-1])\n",
    "    all_branch_outputs.append( x )\n",
    "\n",
    "    # merge the branches together\n",
    "    deep_branch = concatenate(all_branch_outputs)\n",
    "    deep_branch = Dense(units=50,activation='relu')(deep_branch)\n",
    "    deep_branch = Dense(units=40,activation='relu')(deep_branch)\n",
    "    deep_branch = Dense(units=30,activation='relu')(deep_branch)\n",
    "    deep_branch = Dense(units=20,activation='relu')(deep_branch)\n",
    "    deep_branch = Dense(units=10,activation='relu')(deep_branch)\n",
    "\n",
    "    final_branch = concatenate([wide_branch, deep_branch])\n",
    "    final_branch = Dense(units=1,activation='sigmoid')(final_branch)\n",
    "\n",
    "    model2 = Model(inputs=all_inputs, outputs=final_branch)\n",
    "\n",
    "    model2.compile(optimizer='adagrad',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model2.fit(X_ints_train+ [X_train_num],\n",
    "            y_train, epochs=10, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Deeper Final Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to create separate sequential models for each embedding\n",
    "import tensorflow as tf\n",
    "with tf.device('/GPU:0'):\n",
    "    embed_branches = []\n",
    "    X_ints_train = []\n",
    "    X_ints_test = []\n",
    "    all_inputs = []\n",
    "    all_branch_outputs = []\n",
    "\n",
    "    for cols in cross_columns:\n",
    "        # encode crossed columns as ints for the embedding\n",
    "        enc = LabelEncoder()\n",
    "\n",
    "        # create crossed labels\n",
    "        # needs to be commented better, Eric!\n",
    "        X_crossed_train = df_train[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "        X_crossed_test = df_test[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "\n",
    "        enc.fit(np.hstack((X_crossed_train.values,  X_crossed_test.values)))\n",
    "        X_crossed_train = enc.transform(X_crossed_train)\n",
    "        X_crossed_test = enc.transform(X_crossed_test)\n",
    "        X_ints_train.append( X_crossed_train )\n",
    "        X_ints_test.append( X_crossed_test )\n",
    "\n",
    "        # get the number of categories\n",
    "        N = max(X_ints_train[-1]+1) # same as the max(df_train[col])\n",
    "\n",
    "        # create embedding branch from the number of categories\n",
    "        inputs = Input(shape=(1,),dtype='int32', name = '_'.join(cols))\n",
    "        all_inputs.append(inputs)\n",
    "        x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "        x = Flatten()(x)\n",
    "        all_branch_outputs.append(x)\n",
    "\n",
    "    # merge the branches together\n",
    "    wide_branch = concatenate(all_branch_outputs)\n",
    "\n",
    "    # reset this input branch\n",
    "    all_branch_outputs = []\n",
    "    # add in the embeddings\n",
    "    for col in categorical_headers_ints:\n",
    "        # encode as ints for the embedding\n",
    "        X_ints_train.append( df_train[col].values )\n",
    "        X_ints_test.append( df_test[col].values )\n",
    "\n",
    "        # get the number of categories\n",
    "        N = max(X_ints_train[-1]+1) # same as the max(df_train[col])\n",
    "\n",
    "        # create embedding branch from the number of categories\n",
    "        inputs = Input(shape=(1,),dtype='int32', name=col)\n",
    "        all_inputs.append(inputs)\n",
    "        x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "        x = Flatten()(x)\n",
    "        all_branch_outputs.append(x)\n",
    "\n",
    "    # also get a dense branch of the numeric features\n",
    "    all_inputs.append(Input(shape=(X_train_num.shape[1],),sparse=False,name='numeric_data'))\n",
    "    x = Dense(units=20, activation='relu')(all_inputs[-1])\n",
    "    all_branch_outputs.append( x )\n",
    "\n",
    "    # merge the branches together\n",
    "    deep_branch = concatenate(all_branch_outputs)\n",
    "    deep_branch = Dense(units=50,activation='relu')(deep_branch)\n",
    "    deep_branch = Dense(units=10,activation='relu')(deep_branch)\n",
    "\n",
    "    final_branch = concatenate([wide_branch, deep_branch])\n",
    "    final_branch = Dense(units=50,activation='sigmoid')(final_branch)\n",
    "    final_branch = Dense(units=1,activation='sigmoid')(final_branch)\n",
    "\n",
    "    model3 = Model(inputs=all_inputs, outputs=final_branch)\n",
    "\n",
    "    model3.compile(optimizer='Adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model3.fit(X_ints_train+ [X_train_num],\n",
    "            y_train, epochs=10, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = np.round(model.predict(X_ints_test + [X_test_num]))\n",
    "print(mt.confusion_matrix(y_test,yhat),mt.accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(model3).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLearn MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "df_dummies = pd.get_dummies(df, columns=categorical_headers)\n",
    "\n",
    "df_dummies_train, df_dummies_test = train_test_split(df_dummies, test_size=0.2, stratify=df_dummies['readmitted'])\n",
    "y_train_sk = df_dummies_train['readmitted']\n",
    "y_test_sk = df_dummies_test['readmitted']\n",
    "df_dummies_train.drop(['readmitted'], axis=1, inplace=True)\n",
    "df_dummies_test.drop(['readmitted'], axis=1, inplace=True)\n",
    "\n",
    "clf = MLPClassifier()\n",
    "clf.fit(X=df_dummies_train.values, y=np.asarray(y_train_sk.values, dtype=\"|S6\"))\n",
    "yhat_sk = clf.predict(df_dummies_test.values)\n",
    "print(mt.confusion_matrix(y_test_sk.values.astype(int),yhat_sk.astype(int)),mt.recall_score(y_test_sk.values.astype(int),yhat_sk.astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I'm just tryna visualize some weights tbh\n",
    "We'll see how this goes. I'm just dicking around pretty much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train = df_train.sample(frac=0.1)\n",
    "sample_test = df_test.sample(frac=0.1)\n",
    "\n",
    "y_train_sample = sample_train['readmitted']\n",
    "y_test_sample = sample_test['readmitted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to create separate sequential models for each embedding\n",
    "import tensorflow as tf\n",
    "with tf.device('/GPU:0'):\n",
    "    embed_branches = []\n",
    "    X_ints_train = []\n",
    "    X_ints_test = []\n",
    "    all_inputs = []\n",
    "    all_branch_outputs = []\n",
    "\n",
    "    for cols in cross_columns:\n",
    "        # encode crossed columns as ints for the embedding\n",
    "        enc = LabelEncoder()\n",
    "\n",
    "        # create crossed labels\n",
    "        # needs to be commented better, Eric!\n",
    "        X_crossed_train = sample_train[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "        X_crossed_test = sample_test[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "\n",
    "        enc.fit(np.hstack((X_crossed_train.values,  X_crossed_test.values)))\n",
    "        X_crossed_train = enc.transform(X_crossed_train)\n",
    "        X_crossed_test = enc.transform(X_crossed_test)\n",
    "        X_ints_train.append( X_crossed_train )\n",
    "        X_ints_test.append( X_crossed_test )\n",
    "\n",
    "        # get the number of categories\n",
    "        N = max(X_ints_train[-1]+1) # same as the max(df_train[col])\n",
    "\n",
    "        # create embedding branch from the number of categories\n",
    "        inputs = Input(shape=(1,),dtype='int32', name = '_'.join(cols))\n",
    "        all_inputs.append(inputs)\n",
    "        x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "        x = Flatten()(x)\n",
    "        all_branch_outputs.append(x)\n",
    "\n",
    "    # merge the branches together\n",
    "    wide_branch = concatenate(all_branch_outputs)\n",
    "    print(wide_branch.get_weights())\n",
    "\n",
    "    # reset this input branch\n",
    "    all_branch_outputs = []\n",
    "    # add in the embeddings\n",
    "    for col in categorical_headers_ints:\n",
    "        # encode as ints for the embedding\n",
    "        X_ints_train.append( sample_train[col].values )\n",
    "        X_ints_test.append( sample_test[col].values )\n",
    "\n",
    "        # get the number of categories\n",
    "        N = max(X_ints_train[-1]+1) # same as the max(df_train[col])\n",
    "\n",
    "        # create embedding branch from the number of categories\n",
    "        inputs = Input(shape=(1,),dtype='int32', name=col)\n",
    "        all_inputs.append(inputs)\n",
    "        x = Embedding(input_dim=N, output_dim=int(np.sqrt(N)), input_length=1)(inputs)\n",
    "        x = Flatten()(x)\n",
    "        all_branch_outputs.append(x)\n",
    "\n",
    "    # also get a dense branch of the numeric features\n",
    "    all_inputs.append(Input(shape=(X_train_num_sample.shape[1],),sparse=False,name='numeric_data'))\n",
    "    x = Dense(units=20, activation='relu')(all_inputs[-1])\n",
    "    all_branch_outputs.append( x )\n",
    "\n",
    "    # merge the branches together\n",
    "    deep_branch = concatenate(all_branch_outputs)\n",
    "    deep_branch = Dense(units=50,activation='relu')(deep_branch)\n",
    "    deep_branch = Dense(units=10,activation='relu')(deep_branch)\n",
    "\n",
    "    final_branch = concatenate([wide_branch, deep_branch])\n",
    "    final_branch = Dense(units=1,activation='sigmoid')(final_branch)\n",
    "\n",
    "    model4 = Model(inputs=all_inputs, outputs=final_branch)\n",
    "\n",
    "    model4.compile(optimizer='adagrad',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model4.fit(X_ints_train+ [X_train_num_sample],\n",
    "            y_train_sample, epochs=10, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
